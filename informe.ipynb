{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 2 - Árboles de decisión - Evaluación\n",
    "\n",
    "### Grupo 13:\n",
    "     - J. Aguirre  C.I: 4.773.509-6\n",
    "     - A. Collazo C.I: 4.455.617-4\n",
    "     - G. Núnez C.I: 4.785.081-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El principal objetivo de esta tarea es implementar una versión extendida del algoritmo ID3 que sea capaz de procesar atributos numéricos y construir árboles de decisión a partir de esta característica; como también, poder evaluar los resultados obtenidos utilizando distintas medidas.\n",
    "\n",
    "En la primera parte del análisis se utilizará el conjunto de datos *Iris* proporcionado por el equipo docente, que consta de tres clases de 50 instancias cada una y cuatro atributos numéricos. En la segunda parte se utilizará el conjunto de datos *Covertype*, también proporcionado por el equipo docente, que consta de 581012 instancias distribuidas entre siete clases y 54 atributos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente sección se mencionan las decisiones de diseño utilizadas para la construcción de cada una de las partes del presente laboratorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocesamiento de los datos\n",
    "\n",
    "El preprocesamiento -como su nombre lo indica- consiste en preparar el conjunto de datos para luego ser procesado.\n",
    "\n",
    "En una primera etapa se particiona el conjunto de datos en dos subconjuntos más pequeños, es decir, el primero consta del 80% de las instancias originales y se utiliza para entrenar (la construcción del árbol de decisión), mientras que el segundo subconjunto consta del restante 20% de las instancias y se utiliza para evaluar el modelo construido. Notar que al discriminar las instancias en dos subconjuntos disjuntos se evita que el árbol se ajuste por completo a la totalidad del conjunto original, y de esta forma, es posible evaluar el árbol construido mediante las instancias que no fue utilizadas en su construcción. Como la cantidad de instancias del conjunto *Iris* está equilibrada entre sus tres clases, la discriminación se realiza seleccionando al azar 80% de las instancias de cada clase, es decir, se seleccionan 120 de las 150 instancias, siendo 40 instancias de cada clase. En el caso del conjunto *Covertype*, la cantidad de instancias no está equilibrada por lo que la selección de datos requiere obtener un 80% de las instancias de cada clase, y de esta forma, seleccionar el 80% del total el conjunto manteniendo las proporciones. Análogamente el restante 20% de las tuplas se utilizan para evaluar el modelo.\n",
    "\n",
    "Una vez discriminados ambos subconjuntos se procede a la etapa de preprocesamiento propiamente dicha. El preprocesamiento consiste en calcular la entropía y la ganancia del conjunto para cada atributo, y así, ordenarlos según su ganancia. Debido al reordenamiento de los atributos ponderados por su ganancia, es necesario también reordenar los valores de cada instancia de ambos subconjuntos para que coincida con el orden de los atributos. A continuación, se visualiza un ejemplo.\n",
    "\n",
    "Sea el conjunto de atributos: {$A, B, C, D$} y sea el subconjunto de datos de entrenamiento: {{$a_1, b_1, c_1, d_1, n_1$}, {$a_2, b_2, c_2, d_2, n_2$}}; siendo respectivamente $a_n, b_n, c_n$ y $d_n$ el valor de la instancia $n_n$ correspondiente al atributo $A, B, C$ y $D$. Suponiendo que una vez calculada la ganancia de cada atributo el reordenamiento se realiza de la forma: {$B, C, A, D$}, entonces cada instancia del subconjunto se reordena de la forma: {{$b_1, c_1, a_1, d_1, n_1$}, {$b_2, c_2, a_2, d_2, n_2$}}, de modo tal que el orden de los valores de cada instancia coincide con el orden de los atributos, y por ende, también con los niveles del árbol.\n",
    "\n",
    "Notar que la ganancia con la que se determina el orden de los atributos se calcula por única vez y utilizando todas las instancias del subconjunto de entrenamiento, lo que en principio podría resultar poco preciso. A continuación, se explica por qué.\n",
    "\n",
    "Utilizando el mismo conjunto de atributos y el mismo subconjunto de datos de entrenamiento del ejemplo anterior, suponga que luego de calcular la ganancia de cada atributo se determina el siguiente orden: {$B, C, A, D$}. A continuación, se ordenan los valores de todas las instancias del subconjunto para que coincidan, por lo que se tiene: {{$b_1, c_1, a_1, d_1, n_1$}, {$b_2, c_2, a_2, d_2, n_2$}}. Hasta aquí es el mismo procedimiento del ejemplo anterior. Ahora bien, suponga que luego de filtrar las instancias por el atributo $B$, se decide calcular nuevamente la ganancia de cada atributo utilizando las restantes instancias del subconjunto. En este punto existen dos posibilidades. Por un lado, podría pasar que la cantidad de instancias permanezca invariante, en cuyo caso, la ganancia de cada atributo sería la misma. Por otro lado, podría pasar que la cantidad de instancias haya disminuido consecuencia del filtrado, provocando que la ganancia de los restantes atributos en el nuevo subconjunto de datos sea distinta. Por ejemplo, suponga que al recalcular la ganancia de los atributos el orden es: {$D, C, A$}, siendo el orden de los valores de las instancias: {{$d_1, c_1, a_1, n_1$}, {$d_2, c_2, a_2, n_2$}}. Si se unifica ambos cálculos en un orden general se tiene: {{$b_1, d_1, c_1, a_1, n_1$}, {$b_2, d_2, c_2, a_2, n_2$}} y no {{$b_1, c_1, a_1, d_1, n_1$}, {$b_2, c_2, a_2, d_2, n_2$}} como se calculó en un principio.\n",
    "\n",
    "Para clasificar las instancias a partir de atributos numéricos es necesario especificar un criterio. Una opción válida -elegida para el presente laboratorio- es separar los valores en cuartiles. Esta decisión permite distribuir el conjunto de datos de manera uniforme y eliminar el ruido ocasionado por datos poco frecuentes (a diferencia de separar en promedios).\n",
    "\n",
    "El último aspecto a tener en cuenta es que 44 de los 54 atributos del conjunto de datos Covertype son discretos (binarios), por lo que el enfoque para filtrar las instancias es ligeramente distinto. La primera opción -de hecho, la más performante en cuanto a costo computacional- es interpretar los 44 atributos como un número decimal en notación binaria, y así, utilizar los 44 atributos como un único atributo numérico. Dado que no se conoce el contexto (la semántica de cada uno), se optó por evitar la interpretación numérica con puntos de corte en pos de no agrupar valores anómalos o eventualmente imposibles. En contrapartida, los 44 atributos binarios se consideran como atributos independientes.\n",
    "\n",
    "\n",
    "\n",
    "### 2.2 Procesamiento de los datos\n",
    "\n",
    "El procesamiento de datos consiste en construir el árbol de decisión a partir del subconjunto de instancias de entrenamiento. Debido a los diferentes enfoques de las tres secciones se construyó un módulo independiente para cada sección. Para la sección a) se construyó el módulo ID3.py, para la sección b) se construyó el módulo Procesamiento.py, y para la sección c) se construyó el módulo Procesamiento.py. Cabe mencionar que cada módulo se encuentra adjunto dentro su respectiva carpeta.\n",
    "\n",
    "Para representar el árbol de decisión se utilizó la clase llamada Node siendo cada instancia de esta, o una hoja, o un nodo intermedio (de ahora en adelante llamada nodo). El tipo queda determinado por la variable interna leaf, la cual toma el valor verdadero en caso de representar una hoja, o falso en caso contrario. A modo de relacionar la representación algorítmica con la realidad, diremos que los nodos intermedios del árbol representan los atributos mientras que las hojas representan los distintos valores del conjunto de datos.\n",
    "\n",
    "En el caso del árbol multiclase cada hoja contendrá el valor de alguna de las clases del conjunto, por ejemplo, en el conjunto *Iris: Iris-setosa, Iris-versicolor*, entre otras, mientras que en el árbol uniclase cada hoja contendrá el valor *verdadero* o *falso* según corresponda su valor con la clase del árbol. A continuación, se visualiza una sección de un árbol multiclase para el conjunto *Iris*.\n",
    "\n",
    "<img src=\"arbol1.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "*Árbol 1: Representación de un árbol multiclase para el conjunto de datos Iris. Notar que petal_width, petal_length, sepal_length y sepal_width corresponden a los atributos del conjunto y, por ende, a los nodos del árbol, mientras que Iris-setosa, Iris-versicolor e Iris-virginica corresponden a las hojas del árbol.*\n",
    "\n",
    "A continuación, se visualiza una sección de un árbol binario para el conjunto *Iris*. Cabe mencionar que los subconjuntos de datos utilizados para construir ambos ejemplos son distintos y por ello no existe conexión alguna entre ellos.\n",
    "\n",
    "<img src=\"arbol2.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "*Árbol 2: Representación de un árbol binario para el conjunto de datos Iris. Notar que petal_width, petal_length, sepal_length y sepal_width corresponden a los atributos del conjunto y, por ende, a los nodos del árbol, mientras que True y False corresponden a las hojas del árbol.*\n",
    "\n",
    "Dado que *Árbol 2* fue construido para una clase particular del conjunto, el hecho de que una hoja sea *falso* significa que la combinación de atributos desde la raíz hasta esa hoja no se corresponde con esa clase. Análogamente para *verdadero*.\n",
    "Además, se imprime la probabilidad de acierto para la instancia. Este resultado se utilizará para la votación entre los tres árboles.\n",
    "\n",
    "### 2.3 Postprocesamiento de los datos\n",
    "\n",
    "El postprocesamiento de datos consiste en analizar la calidad del árbol de decisión mediante el subconjunto de instancias de verificación. Para cada instancia del subconjunto se recorre el árbol desde la raíz hasta una hoja, eligiendo en cada nivel la rama (cuartil) que mejor se ajusta a su valor.\n",
    "\n",
    "Al toparse con una hoja en un árbol multiclase simplemente verifica que su valor coincida con el de la hoja. En caso afirmativo significa que la decisión fue correcta, y en caso contrario significa que la decisión fue errónea.\n",
    "En un árbol uniclase el procedimiento es similar a excepción de que existen más alternativas. A continuación, se detallan los posibles escenarios.\n",
    "\n",
    "- Si la hoja tiene el valor verdadero, y el valor de la instancia de verificación coincide con el de la clase del árbol, entonces la decisión fue correcta (*verdadero positivo*).\n",
    "- Si la hoja tiene el valor verdadero, y el valor de la instancia de verificación no coincide con el de la clase del árbol, entonces la decisión fue errónea (*falso positivo*).\n",
    "- Si la hoja tiene el valor falso, y el valor de la instancia de verificación coincide con el de la clase del árbol, entonces la decisión fue errónea (*falso negativo*).\n",
    "- Si la hoja tiene el valor falso, y el valor de la instancia de verificación no coincide con el de la clase del árbol, entonces la decisión fue correcta (*verdadero negativo*).\n",
    "\n",
    "A continuación, se presenta el pseudocódigo que muestra la operativa anteriormente descripta:\n",
    "\n",
    "\n",
    "```python\n",
    "    def pre_procesamiento():\n",
    "        generar_train()\n",
    "        generar_test()\n",
    "        calcular_entropia_total()\n",
    "        calcular_puntos_corte() # si corresponde\n",
    "        calcular_ganancia_atributos()\n",
    "        ordenar_atributos()\n",
    "\n",
    "    def procesamiento()\n",
    "        id3()\n",
    "\n",
    "    def post_procesamiento(arbol)\n",
    "        resultados = [0,0,0,0] # v_pos | v_neg | f_pos | f_neg \n",
    "        para tupla en test:\n",
    "            resultados.append(clasificar(tupla, arbol))\n",
    "        return resultados\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Parte a - Extensión del algoritmo ID3 con valores numéricos\n",
    "\n",
    "Como se mencionó anteriormente, de las 150 instancias de conjunto Iris se utilizaron 120 para construir el árbol, y las restantes 30 para evaluar el modelo. Notar que la selección de las instancias de cada subconjunto se realiza de forma aleatoria. Además, cada atributo se particionó en cuatro cuartiles según se indica en el siguiente ejemplo. Cabe mencionar que los valores de *Figura 1* son meramente ilustrativos.\n",
    "\n",
    "<table>\n",
    "        <caption>Figura 1: Se visualiza un ejemplo de la división de los atributos en cuatro cuartiles, donde por ejemplo $i1 = 0,50$ pertenece al tercer cuartil mientras que $i2 = 0,91$ e $i3 = 0,75$ pertenecen al cuarto cuartil.</caption>\n",
    "    <tr><td><img src=\"Cuartiles.png\" alt=\"Drawing\" style=\"width: 400px;\"/></td></tr>\n",
    "</table>    \n",
    " \n",
    "A partir del árbol obtenido y el subconjunto de verificación se realizaron pruebas para conocer el desempeño en la clasificación de cada una de las clases. Cada prueba fue realizada con un subconjunto distinto. Los resultados se visualizan en *Tabla 1*.\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "  <caption>Tabla 1:Se visualiza el resultado de las diez pruebas realizadas.</caption>  \n",
    "  <tr>\n",
    "    <th>Identificador de prueba</th>\n",
    "    <th>Instancias bien clasificadas</th>\n",
    "    <th>Instancias mal clasificadas</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>26</td>\n",
    "    <td>4</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>27</td>\n",
    "    <td>3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>26</td>\n",
    "    <td>4</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>28</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>28</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>26</td>\n",
    "    <td>4</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>27</td>\n",
    "    <td>3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>25</td>\n",
    "    <td>5</td>\n",
    "  </tr>\n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <td>9</td>\n",
    "    <td>29</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>27</td>\n",
    "    <td>3</td>\n",
    "  </tr>   \n",
    "</table>\n",
    "\n",
    "A continuación, se detalla la matriz de confusión para cada una de las pruebas realizadas donde se visualiza los resultados clasificados (correctamente o no) según cada clase.\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>#1</th>\n",
    "    <th>Iris-setosa</th>\n",
    "    <th>Iris-versicolor</th>\n",
    "    <th>Iris-virginica</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-setosa</td>\n",
    "    <td>9</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>Iris-versicolor</td>\n",
    "    <td>1</td>\n",
    "    <td>8</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-virginica</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>9</td>\n",
    "  </tr>     \n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>#2</th>\n",
    "    <th>Iris-setosa</th>\n",
    "    <th>Iris-versicolor</th>\n",
    "    <th>Iris-virginica</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-setosa</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>Iris-versicolor</td>\n",
    "    <td>1</td>\n",
    "    <td>9</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-virginica</td>\n",
    "    <td>0</td>\n",
    "    <td>3</td>\n",
    "    <td>7</td>\n",
    "  </tr>     \n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>#3</th>\n",
    "    <th>Iris-setosa</th>\n",
    "    <th>Iris-versicolor</th>\n",
    "    <th>Iris-virginica</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-setosa</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>Iris-versicolor</td>\n",
    "    <td>0</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-virginica</td>\n",
    "    <td>1</td>\n",
    "    <td>3</td>\n",
    "    <td>6</td>\n",
    "  </tr>     \n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>#4</th>\n",
    "    <th>Iris-setosa</th>\n",
    "    <th>Iris-versicolor</th>\n",
    "    <th>Iris-virginica</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-setosa</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>Iris-versicolor</td>\n",
    "    <td>0</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-virginica</td>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "    <td>7</td>\n",
    "  </tr>     \n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>#5</th>\n",
    "    <th>Iris-setosa</th>\n",
    "    <th>Iris-versicolor</th>\n",
    "    <th>Iris-virginica</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-setosa</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>Iris-versicolor</td>\n",
    "    <td>0</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-virginica</td>\n",
    "    <td>0</td>\n",
    "    <td>3</td>\n",
    "    <td>7</td>\n",
    "  </tr>     \n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>#6</th>\n",
    "    <th>Iris-setosa</th>\n",
    "    <th>Iris-versicolor</th>\n",
    "    <th>Iris-virginica</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-setosa</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>Iris-versicolor</td>\n",
    "    <td>0</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-virginica</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "    <td>7</td>\n",
    "  </tr>     \n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>#7</th>\n",
    "    <th>Iris-setosa</th>\n",
    "    <th>Iris-versicolor</th>\n",
    "    <th>Iris-virginica</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-setosa</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>Iris-versicolor</td>\n",
    "    <td>2</td>\n",
    "    <td>7</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-virginica</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "    <td>10</td>\n",
    "  </tr>     \n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>#8</th>\n",
    "    <th>Iris-setosa</th>\n",
    "    <th>Iris-versicolor</th>\n",
    "    <th>Iris-virginica</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-setosa</td>\n",
    "    <td>8</td>\n",
    "    <td>2</td>\n",
    "    <td>0</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>Iris-versicolor</td>\n",
    "    <td>0</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-virginica</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "    <td>7</td>\n",
    "  </tr>     \n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>#9</th>\n",
    "    <th>Iris-setosa</th>\n",
    "    <th>Iris-versicolor</th>\n",
    "    <th>Iris-virginica</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-setosa</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>Iris-versicolor</td>\n",
    "    <td>0</td>\n",
    "    <td>9</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-virginica</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "    <td>10</td>\n",
    "  </tr>  \n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>#10</th>\n",
    "    <th>Iris-setosa</th>\n",
    "    <th>Iris-versicolor</th>\n",
    "    <th>Iris-virginica</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-setosa</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>Iris-versicolor</td>\n",
    "    <td>0</td>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris-virginica</td>\n",
    "    <td>0</td>\n",
    "    <td>2</td>\n",
    "    <td>8</td>\n",
    "  </tr>     \n",
    "</table>\n",
    "\n",
    "Definimos la línea base para el acierto de la siguiente manera:\n",
    "\n",
    "LB = max \\{  \\%50, max $(\\frac{|c_i|}{|train set|}$) \\}, $c_i \\in Clases(target)$\n",
    "\n",
    "Se puede apreciar que el acierto oscila entre 83,3% y 96,6% superando la línea base, por lo que el modelo obtenido es considerado correcto.\n",
    "En cuanto al comportamiento del algoritmo entre las clases, se observa que las clasificaciones erróneas involucran a las clases *Iris-versicolor* e *Iris-virginica*. Esto podría explicarse en base a la similitud que tienen las mismas (según la descripción del conjunto de datos, *Iris-setosa* es linealmente separable de *Iris-virginica* e *Iris-versicolor*).\n",
    "\n",
    "\n",
    "### 3.2 Parte b - Una clase vs. el resto\n",
    "\n",
    "Luego de realizar algunas modificaciones al algoritmo de la sección 3.1 para crear un árbol de decisión por cada clase del conjunto, se procesó nuevamente el conjunto de datos *Iris*.\n",
    "Se realizaron una serie de pruebas cada una con un subconjunto distinto seleccionado mediante el mismo procedimiento de la parte anterior.\n",
    "\n",
    "Los resultados obtenidos se muestran a continuación. Las medidas seleccionadas para la evaluación son: *precisión, recuperación, fall-off, F1 y acierto*. La medida *F1* fue la elegida para medir el éxito, ya que no se tiene mayor información acerca de la ponderación entre recuperación y precisión para el contexto.\n",
    "\n",
    "<table>\n",
    "  <caption>Tabla 2: Iris-setosa vs el resto.</caption>  \n",
    "  <tr>\n",
    "    <th>Vp</th>\n",
    "    <th>Vn</th>\n",
    "    <th>Fp</th>\n",
    "    <th>Fn</th>\n",
    "    <th>Precisión</th>\n",
    "    <th>Recuperación</th>\n",
    "    <th>Fall-off</th>\n",
    "    <th>F1</th>\n",
    "    <th>Acierto</th>     \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.90</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td>0.94</td>\n",
    "    <td>0.96</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.90</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td>0.94</td>\n",
    "    <td>0.96</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>14</td>\n",
    "    <td>6</td>\n",
    "    <td>2</td>\n",
    "    <td>0.57</td>\n",
    "    <td>0.80</td>\n",
    "    <td>0.30</td>\n",
    "    <td>0.66</td>\n",
    "    <td>0.73</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>18</td>\n",
    "    <td>2</td>\n",
    "    <td>0</td>\n",
    "    <td>0.83</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.93</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>18</td>\n",
    "    <td>2</td>\n",
    "    <td>0</td>\n",
    "    <td>0.83</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.93</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>17</td>\n",
    "    <td>3</td>\n",
    "    <td>0</td>\n",
    "    <td>0.76</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.15</td>\n",
    "    <td>0.86</td>\n",
    "    <td>0.90</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>0.90</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.05</td>\n",
    "    <td>0.95</td>\n",
    "    <td>0.96</td>\n",
    "  </tr>   \n",
    "</table>\n",
    "\n",
    "<table>\n",
    "  <caption>Tabla 3: Iris-versicolor vs el resto.</caption>  \n",
    "  <tr>\n",
    "    <th>Vp</th>\n",
    "    <th>Vn</th>\n",
    "    <th>Fp</th>\n",
    "    <th>Fn</th>\n",
    "    <th>Precisión</th>\n",
    "    <th>Recuperación</th>\n",
    "    <th>Fall-off</th>\n",
    "    <th>F1</th>\n",
    "    <th>Acierto</th>     \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>0.90</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.05</td>\n",
    "    <td>0.95</td>\n",
    "    <td>0.96</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.05</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.93</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>18</td>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "    <td>0.81</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.85</td>\n",
    "    <td>0.90</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>4</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.60</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td>0.75</td>\n",
    "    <td>0.86</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "    <td>0.88</td>\n",
    "    <td>0.80</td>\n",
    "    <td>0.05</td>\n",
    "    <td>0.84</td>\n",
    "    <td>0.90</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>0.90</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.05</td>\n",
    "    <td>0.95</td>\n",
    "    <td>0.96</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.90</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td>0.94</td>\n",
    "    <td>0.96</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>18</td>\n",
    "    <td>2</td>\n",
    "    <td>0</td>\n",
    "    <td>0.83</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.93</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.05</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.93</td>\n",
    "  </tr>    \n",
    "</table>\n",
    "\n",
    "<table>\n",
    "  <caption>Tabla 4: Iris-virginica vs el resto.</caption>  \n",
    "  <tr>\n",
    "    <th>Vp</th>\n",
    "    <th>Vn</th>\n",
    "    <th>Fp</th>\n",
    "    <th>Fn</th>\n",
    "    <th>Precisión</th>\n",
    "    <th>Recuperación</th>\n",
    "    <th>Fall-off</th>\n",
    "    <th>F1</th>\n",
    "    <th>Acierto</th>     \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.90</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td>0.94</td>\n",
    "    <td style=\"font-weight:bold\">0.96</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.05</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.93</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.05</td>\n",
    "    <td>0.90</td>\n",
    "    <td>0.93</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>7</td>\n",
    "    <td>0.75</td>\n",
    "    <td>0.30</td>\n",
    "    <td>0.05</td>\n",
    "    <td>0.42</td>\n",
    "    <td>0.73</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>4</td>\n",
    "    <td>0.85</td>\n",
    "    <td>0.60</td>\n",
    "    <td>0.05</td>\n",
    "    <td>0.70</td>\n",
    "    <td>0.83</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>2</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.80</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td>0.88</td>\n",
    "    <td>0.93</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>6</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.40</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td>0.57</td>\n",
    "    <td>0.80</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>0.90</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.05</td>\n",
    "    <td style=\"font-weight:bold\">0.95</td>\n",
    "    <td style=\"font-weight:bold\">0.96</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>20</td>\n",
    "    <td>0</td>\n",
    "    <td>2</td>\n",
    "    <td style=\"font-weight:bold\">1.00</td>\n",
    "    <td>0.80</td>\n",
    "    <td style=\"font-weight:bold\">0.00</td>\n",
    "    <td>0.88</td>\n",
    "    <td>0.93</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>19</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "    <td>0.88</td>\n",
    "    <td>0.80</td>\n",
    "    <td>0.05</td>\n",
    "    <td>0.84</td>\n",
    "    <td>0.90</td>\n",
    "  </tr>    \n",
    "</table>\n",
    "\n",
    "Para clasificar una instancia de forma definitiva se realiza una votación entre los árboles resultantes. Cada uno de los modelos predice su resultado con una probabilidad de acierto asociada. Si existe al menos una positiva se selecciona la de mayor probabilidad de este conjunto. En caso de que ninguno de los tres árboles declare como propia la instancia a clasificar, el resultado de la votación se determina por el árbol de decisión que haya retornado con menor probabilidad, que la instancia no le corresponde.\n",
    "\n",
    "Los resultados de la votación se muestran en *Tabla 5*. En la mencionada tabla se visualiza la cantidad de aciertos  y errores luego de la votación.\n",
    "\n",
    "<table>\n",
    "  <caption>Tabla 5: Se visualiza el resultado de las diez pruebas realizadas.</caption>  \n",
    "  <tr>\n",
    "    <th>Prueba</th>\n",
    "    <th>Aciertos</th>\n",
    "    <th>Errores</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>27</td>\n",
    "    <td>3</td>\n",
    "  </tr>\n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <td>2</td>\n",
    "    <td>29</td>\n",
    "    <td>1</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>28</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>22</td>\n",
    "    <td>8</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>18</td>\n",
    "    <td>12</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>26</td>\n",
    "    <td>4</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>18</td>\n",
    "    <td>12</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>20</td>\n",
    "    <td>10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>19</td>\n",
    "    <td>11</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>26</td>\n",
    "    <td>4</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "En todos los casos el porcentaje de acierto supera el 59% pero se presentan grandes variaciones.\n",
    "\n",
    "### 3.3 Parte c - Aplicación para el conjunto CoverType\n",
    "\n",
    "Para el conjunto *Covertype* se realiz\n",
    "\n",
    "<table>\n",
    "  <caption>Tabla 6: Una clase vs. el resto</caption>  \n",
    "  <tr>\n",
    "    <th>Id. clase</th>\n",
    "    <th>Vp</th>\n",
    "    <th>Vn</th>\n",
    "    <th>Fp</th>\n",
    "    <th>Fn</th>\n",
    "    <th>Precisión</th>\n",
    "    <th>Recuperación</th>\n",
    "    <th>Fall-off</th>\n",
    "    <th>F1</th>\n",
    "    <th>Acierto</th>     \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\">1 - Spruce/Fir</td>  \n",
    "    <td>23987</td>\n",
    "    <td>53770</td>\n",
    "    <td>20064</td>\n",
    "    <td>18381</td>\n",
    "    <td>0.54</td>\n",
    "    <td>0.56</td>\n",
    "    <td>0.27</td>\n",
    "    <td>0.55</td>\n",
    "    <td>0.66</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\">2 - Lodgepole Pine</td>\n",
    "    <td>31785</td>\n",
    "    <td>39371</td>\n",
    "    <td>20171</td>\n",
    "    <td>24875</td>\n",
    "    <td style=\"font-weight:bold\">0.61</td>\n",
    "    <td>0.56</td>\n",
    "    <td>0.33</td>\n",
    "    <td>0.58</td>\n",
    "    <td>0.61</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\">3 - Ponderosa Pine</td>\n",
    "    <td>4501</td>\n",
    "    <td>106163</td>\n",
    "    <td>2888</td>\n",
    "    <td>2650</td>\n",
    "    <td>0.60</td>\n",
    "    <td>0.62</td>\n",
    "    <td>0.02</td>\n",
    "    <td>0.61</td>\n",
    "    <td>0.95</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\">4 - Cottonwood/Willow</td>  \n",
    "    <td>267</td>\n",
    "    <td>115653</td>\n",
    "    <td>0</td>\n",
    "    <td>282</td>\n",
    "    <td>1</td>\n",
    "    <td>0.48</td>\n",
    "    <td style=\"font-weight:bold\">0</td>\n",
    "    <td style=\"font-weight:bold\">0.65</td>\n",
    "    <td style=\"font-weight:bold\">0.99</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\">5 - Aspen</td>  \n",
    "    <td>672</td>\n",
    "    <td>113306</td>\n",
    "    <td>997</td>\n",
    "    <td>1227</td>\n",
    "    <td>0.40</td>\n",
    "    <td>0.35</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.37</td>\n",
    "    <td>0.98</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\">6 - douglas-fir</td>  \n",
    "    <td>1457</td>\n",
    "    <td>111458</td>\n",
    "    <td>1271</td>\n",
    "    <td>2016</td>\n",
    "    <td>0.53</td>\n",
    "    <td>0.41</td>\n",
    "    <td>0.01</td>\n",
    "    <td>0.46</td>\n",
    "    <td>0.97</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\">7 - Krummholz</td>  \n",
    "    <td>3499</td>\n",
    "    <td>108344</td>\n",
    "    <td>3756</td>\n",
    "    <td>603</td>\n",
    "    <td>0.48</td>\n",
    "    <td style=\"font-weight:bold\">0.85</td>\n",
    "    <td>0.03</td>\n",
    "    <td>0.61</td>\n",
    "    <td>0.96</td>\n",
    "  </tr>    \n",
    "</table>\n",
    "\n",
    "<table>\n",
    "  <caption>Tabla 7: Medida macro (multiclase) </caption>  \n",
    "  <tr>\n",
    "    <th>Macro (Iris)</th>\n",
    "    <th>Macro (CoverType)</th>     \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight:bold\">0.896</td>\n",
    "    <td>0.569</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "  <caption>Tabla 8: Medida micro (booleanos) </caption>  \n",
    "  <tr>\n",
    "    <th>Micro (Iris)</th>\n",
    "    <th>Micro (CoverType)</th>     \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight:bold\">0.983</td>\n",
    "    <td>0.570</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados obtenidos en la sección a) de árboles multiclase sobre el conjunto Iris retorna un acierto medio de 89.95%, mayor a la línea base fijada, y por ende el modelo es calificado como exitoso.\n",
    "\n",
    "Luego, en la sección b) la tasa de acierto del bosque y continuando la calificación mencionada anteriormente, el modelo es etiquetado como exitoso, aunque considerablemente menor a las tasas de los árboles individuales, e incluso irregular.  Esto puede deberse al criterio para evaluar el resultado de la votación, ya que cuando más de un árbol clasifiquen positivamente a una misma instancia para su clase y su probabilidad asociada sea igual, esta será asignada arbitrariamente a la primera clase en el orden de votación.\n",
    "\n",
    "Por otra parte, el árbol multiclase construido para el conjunto Covertype retorna un acierto de 59% lo que lo vuelve también exitoso, aunque considerablemente menor que los modelos generados para el conjunto de datos Iris. Aquí el desafío radica en que los datos están desbalanceados, y el criterio seleccionado para abordar los datos discretos puede no haber sido el óptimo.\n",
    "\n",
    "Futuras mejoras a los modelos desarrollados (incrementar su tasa de acierto), pueden desarrollarse en las siguientes líneas:\n",
    "- Calcular las ganancias a nivel de cada nodo, y no en forma de preprocesamiento (como fue desarrollado en la sección de preprocesamiento).\n",
    "- Generar distinta cantidad de ramas para un nodo, a partir de distintos puntos de corte (quintiles, deciles, percentiles) y distintos criterios (promedios, según ganancia generada, otros).\n",
    "- Hacer validación cruzada.\n",
    "- Cambiar el criterio de decisión de los bosques (no quedarse con la primera instancia de probabilidad máxima).\n",
    "- Para el conjunto de datos *Covertype* en particular: experimentar con interpretar los datos discretos de forma continua (aunque está decisión no parece beneficiosa como se mencionó anteriormente) y afrontar el desbalance de las clases a partir de técnicas específicas para ello:  *«oversampling»* y *«undersampling»*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apéndice 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo fue codificado en el lenguaje Python (versión 3.7.2). Es necesario instalar los paquetes *NumPy* (para facilitar los cálculos matemáticos) y *Colorama* (para decorar la salida de la terminal) ya que no pertenecen a los paquetes base del lenguaje. Se debe ejecutar los siguientes comandos en la terminal del dispositivo para instalar los mencionados paquetes.\n",
    "\n",
    " **python  -m  pip  install  numpy**\n",
    " \n",
    " **python  -m  pip  install  colorama**\n",
    "\n",
    "\n",
    "### A\n",
    "\n",
    "La sección a) consta de cuatro archivos explicados según el orden alfabético.\n",
    "\n",
    "- data_set.txt - contiene el conjunto de datos Iris en su totalidad.\n",
    "- ID3.py - contiene el código referente al algoritmo ID3.\n",
    "- main.py - contiene el módulo principal de ejecución.\n",
    "- PreProcessing.py - contiene el código referente al preprocesamiento del conjunto de datos.\n",
    "\n",
    "Para ejecutar el algoritmo de la presente sección basta con ejecutar en la terminal:\n",
    "\n",
    " **python  main.py**\n",
    "\n",
    "\n",
    "### B\n",
    "\n",
    "La sección b) consta de cinco archivos explicados según el orden alfabético.\n",
    "\n",
    "- data_set.txt - contiene el conjunto de datos Iris en su totalidad.\n",
    "- main.py - contiene el módulo principal de ejecución.\n",
    "- PostProcessing.py - contiene el código referente al postprocesamiento del conjunto de datos.\n",
    "- PreProcessing.py - contiene el código referente al preprosamiento del conjunto de datos.\n",
    "- Processing.py - contiene el código referente al algoritmo ID3.\n",
    "\n",
    "Para ejecutar el algoritmo de la presente sección basta con ejecutar en la terminal:\n",
    "\n",
    "**python  main.py**\n",
    "\n",
    "### C\n",
    "\n",
    "La sección c) consta de cinco archivos explicados según el orden alfabético.\n",
    "\n",
    "- data_set.txt - contiene el conjunto de datos Iris en su totalidad.\n",
    "- main.py - contiene el módulo principal de ejecución.\n",
    "- PostProcessing.py - contiene el código referente al postprocesamiento del conjunto de datos.\n",
    "- PreProcessing.py - contiene el código referente al preprosamiento del conjunto de datos.\n",
    "- Processing.py - contiene el código referente al algoritmo ID3.\n",
    "\n",
    "Para ejecutar el algoritmo de la presente sección basta con ejecutar en la terminal:\n",
    "\n",
    " **python  main.py  tipo_arbol**\n",
    "\n",
    "Siendo *tipo_arbol* la modalidad deseada. Para generar un árbol multiclase se debe ingresar -1, mientras que para generar los correspondientes árboles uniclase se debe ingresar cualquier entero distinto de -1.\n",
    "\n",
    "En el caso de las secciones a) y b), el preprocesamiento genera dos archivos llamados *test_set* y *train_set*, los cuales representan el conjunto de instancias de verificación y entrenamiento respectivamente. En el caso de la sección c), la discriminación de tuplas se encuentra calculado de antemano en los archivos *testing_set* y *training_set* (adjuntos en la entrega)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Iris Data Set - R. A. Fisher, 1936 - https://archive.ics.uci.edu/ml/datasets/iris.\n",
    "\n",
    "[2] Covertype Data Set - https://archive.ics.uci.edu/ml/datasets/Covertype.\n",
    "\n",
    "[3] Análisis de datos - Clasificadores lineales - https://www.tamps.cinvestav.mx/~wgomez/diapositivas/RP/Clase06.pdf.\n",
    "\n",
    "[4] Deep Learning unbalanced training data https://towardsdatascience.com/deep-learning-unbalanced-training-data-solve-it-like-this-6c528e9efea6.\n",
    "\n",
    "[5] Understanding Quantiles: definitions and uses https://www.thoughtco.com/what-is-a-quantile-3126239.\n",
    "\n",
    "[6] Accuracy, Precision, Recall or F1 - https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
